[
{
	"uri": "//localhost:1313/",
	"title": "Create a Serverless Chatbot Using Amazon Bedrock, Amazon Kendra, and Your Own Data",
	"tags": [],
	"description": "",
	"content": "Create a Serverless Chatbot Using Amazon Bedrock, Amazon Kendra, and Your Own Data Overall In this workshop, you will embark on a comprehensive journey to create a serverless chatbot using cutting-edge generative AI technologies. You\u0026rsquo;ll leverage Amazon Bedrock, Amazon Kendra, and your own data to build an intelligent conversational interface that can retrieve and generate contextually relevant responses. The workshop will guide you through configuring multiple services including AWS Lambda, API Gateway, and AWS Amplify, and you\u0026rsquo;ll work with various large language models like Claude 3, Mistral, and Llama. By the end of the workshop, you\u0026rsquo;ll have a fully functional chatbot that can interact with enterprise knowledge bases using Retrieval-Augmented Generation (RAG) techniques. This workshop addresses real-world use cases where organizations need intelligent, data-driven conversational interfaces.\nYou\u0026rsquo;ll learn how to solve challenges such as enterprise knowledge discovery, customer support automation, and information retrieval across complex document repositories. The solution you\u0026rsquo;ll build can be applied in numerous business scenarios, including technical support systems, HR knowledge management, compliance document querying, and research information synthesis. By mastering these techniques, you\u0026rsquo;ll be equipped to create AI-powered solutions that can transform how businesses interact with their data, providing faster, more accurate, and context-aware responses to complex queries.\nYou will create the following architecture for this workshop:\nContent Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Overall In this workshop, you will embark on a comprehensive journey to create a serverless chatbot using cutting-edge generative AI technologies. You\u0026rsquo;ll leverage Amazon Bedrock, Amazon Kendra, and your own data to build an intelligent conversational interface that can retrieve and generate contextually relevant responses. The workshop will guide you through configuring multiple services including AWS Lambda, API Gateway, and AWS Amplify, and you\u0026rsquo;ll work with various large language models like Claude 3, Mistral, and Llama. By the end of the workshop, you\u0026rsquo;ll have a fully functional chatbot that can interact with enterprise knowledge bases using Retrieval-Augmented Generation (RAG) techniques. This workshop addresses real-world use cases where organizations need intelligent, data-driven conversational interfaces.\nYou\u0026rsquo;ll learn how to solve challenges such as enterprise knowledge discovery, customer support automation, and information retrieval across complex document repositories. The solution you\u0026rsquo;ll build can be applied in numerous business scenarios, including technical support systems, HR knowledge management, compliance document querying, and research information synthesis. By mastering these techniques, you\u0026rsquo;ll be equipped to create AI-powered solutions that can transform how businesses interact with their data, providing faster, more accurate, and context-aware responses to complex queries.\nThroughout this workshop, you have gained the following knowledge and skills:\nHow to construct generative AI solutions by utilizing Amazon Bedrock and a serverless architecture. How to use AWS SAM to create and deploy generative AI solutions. How to harness the capabilities of Large Language Models through the Retrieval-Augmented Generation (RAG) architecture. The importance of effective data management in the context of generative AI, which includes sourcing, storing, and preprocessing data to enhance the performance and relevance of AI-generated responses. The role of Prompt Engineering in achieving better outcomes. You will create the following architecture for this workshop:\n🧩 Components Users: End-users of the chatbot Web UI: User interface for chatbot interaction AWS Amplify (React, Vue.js): Manages front-end and authentication, specifically using React or Vue.js Amazon API Gateway: Handles API requests AWS Lambda (RAG/KB/LLM Functions): Executes serverless functions for Retrieval-Augmented Generation, Knowledge Base operations, and LLM interactions Amazon Bedrock: Provides access to AI models and services Large Language Models (Claude 3, Mistral, Llama etc.): AI models powering responses Knowledge Bases: Stores structured information Amazon S3: Object storage for documents and data Documents (PDF, CSV, TXT etc.): Various file types for ingestion Amazon Kendra: Intelligent search service Amazon OpenSearch: Vector database and search engine for efficient similarity search Amazon Cognito: User authentication and authorization 🔄 Workflow Users interact with the Web UI Requests routed through API Gateway to Lambda functions Lambda functions use Bedrock for LLM access, RAG operations, and Knowledge Base interactions Knowledge retrieved from Knowledge Bases, S3, Kendra, or OpenSearch System incorporates various document types to enhance the knowledge base ✨ Key Features Scalable, serverless architecture Leverages various LLM models including Claude 3, Mistral, and Llama Incorporates enterprise knowledge through Kendra and custom knowledge bases Secure authentication with Cognito Flexible document ingestion and search capabilities Front-end built with modern frameworks (React or Vue.js) using AWS Amplify This architecture enables a generative AI-powered chatbot solution that can scale with demand while providing intelligent responses based on both LLMs\u0026rsquo; knowledge and enterprise-specific knowledge. The use of React or Vue.js with AWS Amplify ensures a responsive and efficient user interface.\nAmazon Kendra Amazon Kendra is a managed information retrieval and intelligent search service that uses natural language processing and advanced deep learning model. Unlike traditional keyword-based search, Amazon Kendra uses semantic and contextual similarity—and ranking capabilities—to decide whether a text chunk or document is relevant to a retrieval query.\nSource: Amazon Kendra – What is Kendra?\nThis source provides a more detailed explanation of how Amazon Kendra works.\nAmazon Bedrock Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI companies and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.\nWith Amazon Bedrock\u0026rsquo;s serverless experience, you can get started quickly, privately customize foundation models with your own data, and easily and securely integrate and deploy them into your applications using AWS tools without having to manage any infrastructure.\nSource: Amazon Bedrock – What is Amazon Bedrock?\nThis source provides a more detailed explanation of how Amazon Bedrock works.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-launchastack/",
	"title": "Launch a CloudFormation stack",
	"tags": [],
	"description": "",
	"content": "\rSupported Regions: We recommended that you run this workshop in the us-west-2 AWS Region.\nAn AWS CloudFormation template is used to set up lab resources in the AWS Region that you choose. This step is required because later instructions are based on these resources. The CloudFormation template creates the following AWS resources:\nVSCode: VSCode on Amazon EC2 is a cloud-based integrated development environment (IDE) that you can use to write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. In this workshop, you use VSCode editor to deploy a backend application, which is built by using AWS Serverless Application Model (AWS SAM), and also deploy AWS Amplify frontend. Amazon S3: Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-centered applications, and mobile apps. In this workshop, you use an S3 bucket to upload your use case-centric documents. Amazon Kendra indexes these documents to provide Retrieval-Augmented Generation (RAG) answers to questions that you ask. Amazon Kendra: Amazon Kendra is an intelligent enterprise search service that helps you search across different content repositories with built-in connectors. Download the CloudFormation template: Download the CloudFormation template: Download Store the YAML template file in a folder on your local machine. Navigate to AWS CloudFormation Console 🔗 On the CloudFormation console, choose Upload a template file. Select the template that you just downloaded, and then choose Next Give the stack a name, such as chatbot-startup-stack\nKeep other values unchanged, and choose Next\nFor Configure stack options, select I acknowledge\u0026hellip; options and choose Next\nTo deploy the template, choose Submit After the template is deployed, to review the created resources, navigate to CloudFormation Resources, and then select the CloudFormation stack that you created.\n⏳ Template deployment takes 10–15 minutes to complete all AWS resource provisioning.\nCongratulations! You can now proceed to the next task.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-launchvscode/",
	"title": "Launch VSCode on AWS, and Set Up the Environment",
	"tags": [],
	"description": "",
	"content": "Launch VSCode on AWS To set up your environment, open the VSCode environment (a replacement for AWS Cloud9) which is hosted on Amazon EC2.\nOpen AWS CloudFormation console. Open chatbot-startup-stack stack. Open Outputs, Copy VSCodeWorkspaceURL and VSCodeWorkspacePassword password in a notepad. 4. Open a new browser window or tab, enter the VSCode workspace url, and enter password to launch the VSCode editor.\nAfter it is successfully launched, the default theme is white, optionally you can change to different color themes. For example, Settings Icon -\u0026gt; Themes -\u0026gt; Color Theme -\u0026gt; Dark (Visual Studio)\nYour VSCode editor is ready. Congratulations! You can now proceed to next task.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "\rIt is recommended to use an IAM user account with administrative privileges rather than the root account. Some AWS services, such as Knowledge Bases in Amazon Bedrock or other AI-related services, may be restricted or unavailable when using the root account. For security and compatibility reasons, AWS best practices also advise avoiding the use of the root user for daily tasks.\nSupported Regions: We recommended that you run this workshop in the us-west-2 AWS Region.\nIn this workshop, you configure the following AWS services to build a generative AI chatbot.\nAmazon Bedrock Knowledge Bases Amazon Kendra AWS Lambda An AWS CloudFormation template is used to set up lab resources in the AWS Region that you choose. This step is required because later instructions are based on these resources\nCost: Be aware that if you run this workshop in your own AWS account, you will incur costs for resource usage. The Clean Up section in the left navigation pane can help you remove resources from your environment when you are done.\nContent 2.1 Launch a CloudFormation stack 2.2 Launch VSCode on AWS, and Set Up the Environment "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/",
	"title": "Deploy Amazon Bedrock Serverless Application",
	"tags": [],
	"description": "",
	"content": "In this task, you will build and deploy both the backend and frontend components of the application. The backend is deployed as a serverless application using AWS SAM, which creates an Amazon API Gateway, the necessary AWS Lambda functions, a Cognito user pool, and stores login credentials in AWS Secrets Manager.\nThe frontend is built using Vue.js and deployed using AWS Amplify. The frontend UI will call REST-based API calls hosted using Amazon API Gateway, API Gateway invokes AWS Lambda function, function calls Amazon Bedrock APIs. This serverless architecture allows the application to scale efficiently and reduces the operational overhead of managing the underlying infrastructure. The deployment process is automated through a startup.sh script, which handles the end-to-end provisioning of the entire application stack.\nBuild and deploy the chatbot application From the VSCode editor, run the following command to set a key environment variable used throughout the workshop. Replace \u0026lt;chatbot-startup-stack\u0026gt; with the actual stack name from the AWS CloudFormation console.\nThis CFNStackName variable will be referenced later when interacting with the resources provisioned as part of app deployment. And, the environment variable S3BucketName will be used throughout the workshop to store various data sources and knowledge bases required for the application. The backend services will interact with the contents of this S3 bucket to retrieve and process the necessary information for the application\u0026rsquo;s functionality. Ensuring the S3BucketName environment variable is properly set will allow the workshop tasks to seamlessly access and utilize the required data stored in this central location.\nexport CFNStackName=\u0026#34;chatbot-startup-stack\u0026#34; export S3BucketName=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;S3BucketName\u0026#39;].OutputValue\u0026#34; --output text) echo \u0026#34;S3 bucket name: $S3BucketName\u0026#34; To clone the source code for this workshop, run the following command. The application code is hosted in the open-source aws-samples GitHub repository, which contains a variety of sample projects provided by AWS. Cloning the repository will ensure you have the latest version of the code and can easily make any modifications as you progress through the workshop tasks.\ncd ~/environment git clone https://github.com/aws-samples/bedrock-serverless-workshop.git To build and deploy backend and frontend of serverless app, run the following command. The aws-creds.py script is used to create an AWS profile, which is a required step for the subsequent frontend deployment process..\ncd ~/environment/bedrock-serverless-workshop python3 aws-creds.py chmod +x startup.sh ./startup.sh The script is doing following tasks:\nUpgrade OS, install jq software. Build backend using sam build. Deploy backend using sam deploy. Install Amplify and build frontend. Publish the frontend application using Amplify. The script will take anywhere from 2 to 5 minutes to finish. If there is a git alert popup window at some point, choose OK.\nDuring amplify add host, you are prompted with a selection twice, keep the default selections and hit enter. The defaults are, Hosting with Amplify Console (Managed hosting with custom domains, Continuous deployment) for the first option, and Manual deployment for the second option.\nAfter completion, you will see the following image.\nCopy and store the value for amplifyapp url, user_id, and password in a text editor. You use these credentials to sign in to the UI.\nLaunch the amplifyapp url from the web browser, login using above credentials. After a successful login you see the the below home screen. Note, it is not yet ready with source documents, and the chat is not yet functional. In the next task you complete indexing your source documents and test with sample questions. Before you move to next task, run below commands, these are the environment variables required for SAM and Amplify build commands for rest of the lab.\nRun the following commands in the VS Code terminal.\nexport SAMStackName=\u0026#34;sam-$CFNStackName\u0026#34; export AWS_REGION=$(aws configure get region) export KendraIndexID=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;KendraIndexID\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CognitoUserPool\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolClientId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CongnitoUserPoolClientID\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) export PATH=~/.npm-global/bin:$PATH After running, you can check again by using the export command and verifying whether the above variables(AWS_REGION, KendraIndexID, BedrockApiUrl,\u0026hellip;) have been assigned values. If the variables do not have values, check your region to ensure it is set to the us-west-2 AWS Region. If it\u0026rsquo;s incorrect, delete the stack, select the correct region, and run the steps again.\nYou have completed the deployment of the backend and frontend of the chatbot using Amazon Bedrock serverless\n"
},
{
	"uri": "//localhost:1313/4-walkthroughbedrock/",
	"title": "Walkthrough Amazon Bedrock Console",
	"tags": [],
	"description": "",
	"content": "Use Amazon Bedrock to build and scale generative AI applications with FMs. Amazon Bedrock is a fully managed service that makes FMs, from leading AI startups and Amazon, available through an API. You can choose from a wide range of FMs to find the model that is best suited for your use case. With the Amazon Bedrock serverless experience, you can get started quickly, privately customize FMs with your own data, and quickly integrate and deploy them into your applications by using AWS tools without having to manage any infrastructure.\nOpen Amazon Bedrock console Activate model access To complete this workshop, you need to enable access to Claude3, Mistral, and Llama models. Additionally, verify that the Titan Text Embeddings V2 model is enabled; this model is typically available by default, but confirm its status and enable it if necessary. Please ensure you have access to all these required models before proceeding with the workshop.\nAt the upper-left of the Amazon Bedrock console, choose the menu (hamburger) icon. In the navigation pane, choose Model access, and then select Enable specific models. Select the following LLMs: Titan Text Embeddings V2 Claude 3.5 Sonnet Claude 3 Haiku Claude 3 Opus Llama 3.1 8B Instruct 8B Mistral 7B Instruct Choose Next Choose Submit In the current version of this workshop environment, the use of the Claude 3 Opus model is not supported. If you encounter any errors related to Opus, you can safely ignore them and proceed without testing the Opus model. However, if you choose to deploy this application in your own AWS account, you will be able to enable the Opus model without any issues.\nAsk a question At the upper-left of the Amazon Bedrock console, choose the menu (hamburger) icon.\nUnder Test, choose Chat / Text playground.\nYou can also take this time, on the left menu, to navigate around the different options.\nOn the Chat / Text playground, choose Select model. In the Select model popup, choose Anthropic and select Claude 3 Haiku. Choose Apply.\nIn the prompt text box, type question, and then choose Run.\nSample question – List top 10 cities of VietNam by population?\nThe model returns a response similar to this:\nIn the Configurations, You can try randomness by changing parameter like temperature, Top P, Top K, and token length. You can also try with a different questions and experience with different response behavior.\nTemperature is a parameter that controls the level of randomness in the output; the higher the value, the more creative the model becomes. Top P is the cumulative probability threshold for selecting candidate tokens, helping balance between logic and diversity. Top K is the number of highest-probability tokens considered for the next word selection. Max token length is the maximum number of tokens the model will generate in a response to control the output length and cost. 🎉 Congratulations You can now proceed to the next task.\n"
},
{
	"uri": "//localhost:1313/5-multiplellms/",
	"title": "General Chat with Multiple LLMs",
	"tags": [],
	"description": "",
	"content": "\rPort Forwarding is a useful way to redirect network traffic from one IP address - Port to another IP address - Port. With Port Forwarding we can access an EC2 instance located in the private subnet from our workstation.\nYou have successfully deployed the Amazon Bedrock serverless chatbot application, using AWS SAM for the backend and AWS Amplify for the frontend. The frontend serves as a basic user interface for testing the solution with various questions and prompt parameters. In this section, you will update and deploy a LLM Lambda function, this function enables with a general chat with multiple large language models.\nOpen the VSCode editor (as discussed in Task 1). From bedrock-serverless-workshop project, open /lambdas/llmFunctions/llmfunction.py function, copy the below code and update the function code. This function carries the logic to support Claud3 (Haiku, Sonnet etc.), Mistral and Llama models. import boto3 import json import traceback region = boto3.session.Session().region_name def lambda_handler(event, context): boto3_version = boto3.__version__ print(f\u0026#34;Boto3 version: {boto3_version}\u0026#34;) print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) prompt = event_body[\u0026#34;query\u0026#34;] temperature = event_body[\u0026#34;temperature\u0026#34;] max_tokens = event_body[\u0026#34;max_tokens\u0026#34;] model_id = event_body[\u0026#34;model_id\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 try: if model_id == \u0026#39;mistral.mistral-7b-instruct-v0:2\u0026#39;: response = invoke_mistral_7b(model_id, prompt, temperature, max_tokens) elif model_id == \u0026#39;meta.llama3-1-8b-instruct-v1:0\u0026#39;: response = invoke_llama(model_id, prompt, temperature, max_tokens) else: response = invoke_claude(model_id, prompt, temperature, max_tokens) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;answer\u0026#39;: response}) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(stack_trace) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } def invoke_claude(model_id, prompt, temperature, max_tokens): try: instruction = f\u0026#34;Human: {prompt} nAssistant:\u0026#34; bedrock_runtime_client = boto3.client(service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=region) body= { \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: prompt}], } ], } response = bedrock_runtime_client.invoke_model( modelId=model_id, body=json.dumps(body) ) response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) outputs = response_body.get(\u0026#34;content\u0026#34;) completions = [output[\u0026#34;text\u0026#34;] for output in outputs] print(f\u0026#34;completions: {completions[0]}\u0026#34;) return completions[0] except Exception as e: raise def invoke_mistral_7b(model_id, prompt, temperature, max_tokens): try: instruction = f\u0026#34;\u0026lt;s\u0026gt;[INST] {prompt} [/INST]\u0026#34; bedrock_runtime_client = boto3.client(service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=region) body = { \u0026#34;prompt\u0026#34;: instruction, \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, } response = bedrock_runtime_client.invoke_model( modelId=model_id, body=json.dumps(body) ) response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) outputs = response_body.get(\u0026#34;outputs\u0026#34;) print(f\u0026#34;response: {outputs}\u0026#34;) completions = [output[\u0026#34;text\u0026#34;] for output in outputs] return completions[0] except Exception as e: raise def invoke_llama(model_id, prompt, temperature, max_tokens): print(f\u0026#34;Invoking llam model {model_id}\u0026#34; ) print(f\u0026#34;max_tokens {max_tokens}\u0026#34; ) try: instruction = f\u0026#34;[INST]You are a very intelligent bot with exceptional critical thinking, help me answering below question.[/INST]\u0026#34; total_prompt = f\u0026#34;{instruction}\\n{prompt}\u0026#34; print(f\u0026#34;Prompt template {total_prompt}\u0026#34; ) bedrock_runtime_client = boto3.client(service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=region) body = { \u0026#34;prompt\u0026#34;: total_prompt, \u0026#34;max_gen_len\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_p\u0026#34;: 0.9 } response = bedrock_runtime_client.invoke_model( modelId=model_id, body=json.dumps(body) ) response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) print(f\u0026#34;response: {response_body}\u0026#34;) return response_body [\u0026#39;generation\u0026#39;] except Exception as e: raise From VSCode editor, run the following command to build and deploy with new updated lambda code. cd ~/environment/bedrock-serverless-workshop sam build \u0026amp;\u0026amp; sam deploy You can inspect the updated Lambda function using AWS Lambda Console.\nIn this task, you have the opportunity to work with following LLM models, each offering unique functionalities and specializations. These models include:\nanthropic.claude-3-haiku - Claude 3 Haiku is Anthropic’s fastest, most compact model for near-instant responsiveness. Haiku is the best choice for building seamless AI experiences that mimic human interactions. anthropic.claude-3-5-sonnet - Anthropic’s most intelligent and advanced model, Claude 3.5 Sonnet, demonstrates exceptional capabilities across a diverse range of tasks and evaluation. anthropic.claude-3-opus - Opus is a highly intelligent model with reliable performance on complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Use Opus to automate tasks, and accelerate research and development across a diverse range of use cases and industries. mistral.mistral-7b-instruct - Mistral is a highly efficient large language model optimized for high-volume, low-latency language-based tasks. Popular use cases for Mistral are text summarization, structuration, question answering, and code completion meta.llama3-1-8b-instruct - Llama 3.1 8B is best suited for limited computational power and resources. The model excels at text summarization, text classification, sentiment analysis, and language translation requiring low-latency inferencing. Test using UI Return to the browser and open the chatbot webpage. If you are not signed in, use the credentials that you retrieved earlier to sign in. The webpage is displayed as follows: 2. Ask any question by typing in the Query text box and click Ask Question button. You can try with a sample question from the right side panel. Here is a sample question and the response from Claude 3 Haiku. You can try with other LLMs and compare the results.\nList top 10 cities of VietNam by population? Task Complete You can now proceed to next task.\n"
},
{
	"uri": "//localhost:1313/6-usingamazonkendra/",
	"title": "Index the Source Data by Using Amazon Kendra",
	"tags": [],
	"description": "",
	"content": "In the RAG architecture, Amazon Kendra can be leveraged to index and search through a collection of sample documents stored in Amazon S3 and other sources. Users can provide a query or question, and Kendra will perform a similarity search across the indexed content to identify the most relevant information.\nKendra\u0026rsquo;s advanced natural language processing capabilities allow it to understand the user\u0026rsquo;s intent and query, and then retrieve the most pertinent content from the indexed data sources. This enables users to quickly find the information they need, without having to manually sift through large volumes of documents.\nBy integrating Kendra into the \u0026ldquo;Retrieve\u0026rdquo; phase of the RAG architecture, organizations can enhance their overall search and information discovery capabilities, ultimately supporting more effective analysis and generation of insights and responses. Kendra\u0026rsquo;s seamless integration with Amazon S3 simplifies the process of indexing and managing the underlying content, making it a powerful tool within the RAG framework.\nDownload sample documents Download a few sample documents to test this solution. The first document pertains to the May and September 2024 meeting minutes of the Federal Open Market Committee (FOMC). The second document is the 2023 Amazon Sustainability Report. The third document is the 2023 10K report for Henry Schein, a provider of dental services. You can download or use any document to test this solution, or you can bring your own data to conduct tests.\nTo copy the prompt templates and sample documents that you downloaded to the S3 bucket, run the following command.\nOpen the VSCode editor, then run the command in the TERMINAL.\ncd ~/environment/bedrock-serverless-workshop mkdir sample-documents curl https://www.federalreserve.gov/monetarypolicy/files/monetary20240501a1.pdf --output sample-documents/monetary20240501a1.pdf curl https://www.federalreserve.gov/monetarypolicy/files/monetary20240918a1.pdf --output sample-documents/monetary20240918a1.pdf curl https://sustainability.aboutamazon.com/content/dam/sustainability-marketing-site/pdfs/reports-docs/2023-amazon-sustainability-report.pdf --output sample-documents/2023-sustainability-report-amazon.pdf curl https://investor.henryschein.com/static-files/bcc116aa-a576-4756-a722-90f5e2e22114 --output sample-documents/2023-hs1-10k.pdf Upload sample documents and prompt templates To copy prompt templates and sample documents you downloaded to the S3 bucket, run the following command.\ncd ~/environment/bedrock-serverless-workshop aws s3 cp sample-documents s3://$S3BucketName/sample-documents/ --recursive aws s3 cp prompt-engineering s3://$S3BucketName/prompt-engineering/ --recursive After a successful upload, review the Amazon S3 console and open the bucket. You should see something like this:\nIndex the sample documents by using Amazon Kendra The Amazon Kendra index and Amazon S3 data source were created during the initial provisioning for this workshop. In this task, you index all the documents in the S3 data source.\nOpen Amazon Kendra console\nAt the upper-left of the Amazon Kendra console, choose the menu menu ☰ icon, and then, in the navigation pane, choose Indexes. Click on the index name to see the left navigation pane.\nTo start indexing all the documents from the sample-documents folder, select the S3DocsDataSource, and then choose Sync now. The indexing might take a couple minutes. Wait for it to be completed. To query the Amazon Kendra index with a few sample questions, in the left navigation pane, choose Search indexed content, and then ask a question.\nSample question:\nWhat is federal funds rate as of May 2024?? Congratulations You can now proceed to next task.\n"
},
{
	"uri": "//localhost:1313/7-rag/",
	"title": "RAG Chat with Multiple LLMs",
	"tags": [],
	"description": "",
	"content": "You have successfully deployed the Amazon Bedrock serverless chatbot application and enabled model access to the required Large Language Models (LLMs) using the Amazon Bedrock console. Additionally, you have completed the setup of Amazon Kendra.\nRAG-based solution with Amazon Bedrock and LangChain The solution in this workshop is constructed using the Retrieval-Augmented Generation (RAG) approach. RAG is a model architecture that integrates aspects of both retrieval and generation techniques to enhance the quality and relevance of generated text. When you input a question into the question text box, the following backend steps are run to provide you with an answer derived from the document source:\nRetrieval: This process searches through a large corpus of text data to find relevant information or context. During this stage, Amazon Kendra takes the question from the request and searches for the relevant answers and references.\nAugmentation: After retrieving relevant information, the model uses the retrieved context to augment the generation of text. This means that the generated text is influenced by the retrieved information, ensuring that the generated content is contextually appropriate and informative.\nGeneration: Generation in RAG refers to the traditional generative aspect of the model, where it creates new text based on the retrieved and augmented context. This generated text can be in the form of answers, responses, or explanations.\nLangChain: To orchestrate this flow, we employ the LangChain agent in this workshop. LangChain\u0026rsquo;s flexible abstractions and comprehensive toolkit empower developers to harness the capabilities of foundation models (FMs).\nIn this task, you will deploy a RAG (Retrieve, Analyze, Generate) Lambda function to provide a contextual chatbot experience with your data sets. The sample data sets are stored in Amazon S3 and indexed using Amazon Kendra. To orchestrate the flow across user queries, Kendra index, and LLMs, you will be using LangChain as the orchestration tool. The provided Lambda code utilizes the LangChain API to abstract the complex logic required for this integration.\nBy leveraging the power of Amazon Bedrock, Amazon Kendra, and LangChain, you can create a seamless and contextual chatbot experience for your users, allowing them to engage with your data sets in a natural and efficient manner.\nIn this section, you will update and deploy a RAG Lambda function, this function enables with a contextual chat with multiple large language models.\nOpen the VSCode editor.\nFrom bedrock-serverless-workshop project, open /lambdas/ragFunctions/ragfunction.py function, copy the below code and update the function code. This function carries the logic to support Claud3 (Haiku, Sonnet etc.), Mistral and Llama models. import os import json import boto3 from langchain_community.retrievers import AmazonKendraRetriever from langchain_aws import ChatBedrock from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory from langchain.prompts import PromptTemplate from langchain.chains import RetrievalQA import traceback kendra = boto3.client(\u0026#39;kendra\u0026#39;) chain_type = \u0026#39;stuff\u0026#39; KENDRA_INDEX_ID = os.getenv(\u0026#39;KENDRA_INDEX_ID\u0026#39;) S3_BUCKET_NAME = os.environ[\u0026#34;S3_BUCKET_NAME\u0026#34;] refine_prompt_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;This is the original question: {question}\\n\u0026#34; \u0026#34;The existing answer: {existing_answer}\\n\u0026#34; \u0026#34;Now there are some additional texts, (if needed) you can use them to improve your existing answer.\u0026#34; \u0026#34;\\n\\n\u0026#34; \u0026#34;{context_str}\\n\u0026#34; \u0026#34;\\\\nn\u0026#34; \u0026#34;Please use the new passage to further improve your answer.\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) initial_qa_template = ( \u0026#34;Below is an instruction that describes a task. \u0026#34; \u0026#34;Write a response that appropriately completes the request.\\n\\n\u0026#34; \u0026#34;### Instruction:\\n\u0026#34; \u0026#34;The following is background knowledge：\\n\u0026#34; \u0026#34;{context_str}\u0026#34; \u0026#34;\\n\u0026#34; \u0026#34;Please answer this question based on the background knowledge provided above：{question}。\\n\\n\u0026#34; \u0026#34;### Response: \u0026#34; ) def lambda_handler(event, context): print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) question = event_body[\u0026#34;query\u0026#34;] print(f\u0026#34;Query is: {question}\u0026#34;) model_id = event_body[\u0026#34;model_id\u0026#34;] temperature = event_body[\u0026#34;temperature\u0026#34;] max_tokens = event_body[\u0026#34;max_tokens\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; try: if model_id == \u0026#39;mistral.mistral-7b-instruct-v0:2\u0026#39;: llm = get_mistral_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/mistral-prompt-template.txt\u0026#39; elif model_id == \u0026#39;meta.llama3-1-8b-instruct-v1:0\u0026#39;: llm = get_llama_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/llama-prompt-template.txt\u0026#39; else: llm = get_claude_llm(model_id,temperature,max_tokens) PROMPT_TEMPLATE = \u0026#39;prompt-engineering/claude-prompt-template.txt\u0026#39; # Read the prompt template from S3 bucket s3 = boto3.resource(\u0026#39;s3\u0026#39;) obj = s3.Object(S3_BUCKET_NAME, PROMPT_TEMPLATE) prompt_template = obj.get()[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8\u0026#39;) print(f\u0026#34;prompt template: {prompt_template}\u0026#34;) retriever = AmazonKendraRetriever(kendra_client=kendra,index_id=KENDRA_INDEX_ID) if chain_type == \u0026#34;stuff\u0026#34;: PROMPT = PromptTemplate( template=prompt_template, input_variables=[\u0026#34;context\u0026#34;, \u0026#34;question\u0026#34;] ) chain_type_kwargs = {\u0026#34;prompt\u0026#34;: PROMPT} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;stuff\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) elif chain_type == \u0026#34;refine\u0026#34;: refine_prompt = PromptTemplate( input_variables=[\u0026#34;question\u0026#34;, \u0026#34;existing_answer\u0026#34;, \u0026#34;context_str\u0026#34;], template=refine_prompt_template, ) initial_qa_prompt = PromptTemplate( input_variables=[\u0026#34;context_str\u0026#34;, \u0026#34;question\u0026#34;], template=prompt_template, ) chain_type_kwargs = {\u0026#34;question_prompt\u0026#34;: initial_qa_prompt, \u0026#34;refine_prompt\u0026#34;: refine_prompt} qa = RetrievalQA.from_chain_type( llm=llm, chain_type=\u0026#34;refine\u0026#34;, retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs) response = qa(question, return_only_outputs=False) print(\u0026#39;Response\u0026#39;, response) source_documents = response.get(\u0026#39;source_documents\u0026#39;) source_docs = [] previous_source = None previous_score = None response_data = [] #if chain_type == \u0026#34;stuff\u0026#34;: for source_doc in source_documents: source = source_doc.metadata[\u0026#39;source\u0026#39;] score = source_doc.metadata[\u0026#34;score\u0026#34;] if source != previous_source or score != previous_score: source_data = { \u0026#34;source\u0026#34;: source, \u0026#34;score\u0026#34;: score } response_data.append(source_data) previous_source = source previous_score = score response_with_metadata = { \u0026#34;answer\u0026#34;: response.get(\u0026#39;result\u0026#39;), \u0026#34;source_documents\u0026#34;: response_data } return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(response_with_metadata) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(f\u0026#34;stack trace: {stack_trace}\u0026#34;) print(f\u0026#34;error: {str(e)}\u0026#34;) response = str(e) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: response}) } def get_claude_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.95 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_llama_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_gen_len\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_mistral_llm(model_id, temperature, max_tokens): model_kwargs = { \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_k\u0026#34;: 50, \u0026#34;top_p\u0026#34;: 0.9 } llm = ChatBedrock(model_id=model_id, model_kwargs=model_kwargs) return llm def get_memory(): memory = ConversationBufferMemory( memory_key=\u0026#34;chat_history\u0026#34;, input_key=\u0026#34;question\u0026#34;, output_key=\u0026#34;answer\u0026#34;, return_messages=True ) return memory From VSCode terminal, run the following command to build and deploy with new updated lambda code. cd ~/environment/bedrock-serverless-workshop sam build \u0026amp;\u0026amp; sam deploy In this task, you have the opportunity to work with following LLM models, each offering unique functionalities and specializations. These models include:\nanthropic.claude-3-haiku - Claude 3 Haiku is Anthropic’s fastest, most compact model for near-instant responsiveness. Haiku is the best choice for building seamless AI experiences that mimic human interactions. anthropic.claude-3-5-sonnet - Anthropic’s most intelligent and advanced model, Claude 3.5 Sonnet, demonstrates exceptional capabilities across a diverse range of tasks and evaluation. anthropic.claude-3-opus - Opus is a highly intelligent model with reliable performance on complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Use Opus to automate tasks, and accelerate research and development across a diverse range of use cases and industries. mistral.mistral-7b-instruct - Mistral is a highly efficient large language model optimized for high-volume, low-latency language-based tasks. Popular use cases for Mistral are text summarization, structuration, question answering, and code completion meta.llama3-1-8b-instruct - Llama 3.1 8B is best suited for limited computational power and resources. The model excels at text summarization, text classification, sentiment analysis, and language translation requiring low-latency inferencing. Return to the browser and open the chatbot webpage. Click on RAG Chat with LLMs link. The webpage is displayed as follows: Ask any question by typing in the Query text box and click Ask Question button. You can try with a sample question from the right side panel. Here is a sample question and the response from Claude 3 Haiku. You can try with other LLMs and compare the results. What is federal funds rate as of September 2024? "
},
{
	"uri": "//localhost:1313/8-promptengineering/",
	"title": "Prompt Engineering",
	"tags": [],
	"description": "",
	"content": "Prompt engineering is a crucial aspect of the Retrieval-Augmented Generation (RAG) architecture, as it plays a vital role in each stage of the process. In the Retrieval stage, prompt engineering is employed to craft effective queries that retrieve the most relevant information from data sources, such as Amazon S3 documents indexed with Amazon Kendra, accurately capturing the user\u0026rsquo;s intent. During the Analysis stage, prompts guide the processing and extraction of insights from the retrieved data, specifying the type of analysis, level of detail, and desired output format. Finally, in the Generation stage, prompt engineering is essential for leveraging large language models like Claude 3 Sonnet to produce coherent, relevant, and task-specific responses, providing the necessary context, instructions, and guidelines. By mastering prompt engineering, users can optimize the performance of the RAG architecture, ensuring that the retrieved information is relevant, the analysis is insightful, and the generated responses effectively address the user\u0026rsquo;s needs.\nIn this task, you will work on fine-tuning the prompt for the Claude 3 Sonnet large language model. You will experiment with how the response behavior changes based on the prompt template you provide. Initially, you will ask a question with a loosely structured prompt, which may cause the LLM to deviate from the context and provide undesirable output. As you refine the prompt, the response will become more closely aligned with the expected outcome and exhibit less hallucination. This section is still part of the RAG architecture and will utilize the same data sources you employed in the previous task.\nReturn to the browser and open the chatbot webpage. Click on Prompt Engineering link. The webpage is displayed as follows: Try a below question which is not relevant to the source documents you so far indexed. Tell me a story about a fox and tiger, the story must be for a 5 year old and under 100 words. For the prompt template, copy the following prompt and paste it into the Prompt Template text box. This is a loosely structured prompt without any specific direction or guidance provided to the Claude 3.5 Sonnet model. {context} and {question} You will most likely receive a response similar to the one shown below. However, this response does not exist in any of the source documents. Due to the weak prompt structure, the model has hallucinated the response, providing an output that is not grounded in the given context or information. 5. Now, update the prompt with the following fine-tuned version. With this refined prompt, you can expect to receive the desired output.\nHuman: You are an intelligent AI advisor, and provide answers to questions by using fact based information. Use the following pieces of information to provide a concise answer to the question enclosed in \u0026lt;question\u0026gt; tags. Look for the contextual information enclosed in \u0026lt;context\u0026gt; tags. If you don\u0026#39;t know the answer, just say that you don\u0026#39;t know, don\u0026#39;t try to make up an answer. \u0026lt;context\u0026gt;{context}\u0026lt;/context\u0026gt; \u0026lt;question\u0026gt;{question}\u0026lt;/question\u0026gt; The response should be specific and use facts only. Assistant: Observe the response. This time, instead of hallucinating, the model acknowledges that it does not find any relevant context to provide an answer. This is mainly because the prompt directions are clear and obvious to the model, hence leading to the desired output. 7. Lastly, try with a question that is relevant to your knowledge base and observe the response. The response looks more polished and factually correct.\nWhat is federal funds rate as of May 2024? You can experiment with different questions and different prompts until you see concise and consistent responses. Prompt engineering is an iterative process, and you may need to try various options until the responses meet your desired guidelines.\n"
},
{
	"uri": "//localhost:1313/9-knowledgebases/9.1-createknowledgebase/",
	"title": "Create and Test Knowledge Base",
	"tags": [],
	"description": "",
	"content": "Creating an Amazon Bedrock Knowledge Base involves integrating proprietary data into AI applications using Retrieval Augmented Generation (RAG). The process converts text from data sources into vector embeddings using models like Amazon Titan Embeddings, storing them in a vector database such as OpenSearch Serverless. Once set up, the Knowledge Base can be queried to retrieve relevant information, augmenting prompts for foundation models.\nCreate Knowledge Base Open Amazon Bedrock console. At the upper-left of the Amazon Bedrock console, choose the menu ☰ icon. In the navigation pane, choose Knowledge Bases. Choose Create knowledge base, select Knowledge base with vector store Keep the default name. In IAM permissions, choose Use an existing role. From the dropdown, select xxxx-BedrockExecutionRoleForKBs-xxxx role. Select Amazon S3 for the data source. Choose Next. Choose Browse S3. Choose xxxx-s3bucket-xxxx and sample-documents folder. 11. Keep the default chunking configuration.\n12. Choose Next.\n13. For Embeddings model, choose Titan Text Embeddings v2.\n14. For Vector database, choose Quick create\u0026hellip;\n15. For Vector store, choose Amazon OpenSearch Serverless\n16. Choose Create knowledge base.\n17. This may take few minutes to complete. Sync data source with Knowledge base Under Data source, choose your knowledge source (knowledge-base-quick-\u0026hellip;). Choose Sync button.\nAfter few seconds, the knowledge base is ready to test. Test Knowledge base Select your Knowledge base and choose Test knowledge base button. In the right side pane, choose Select model.\nIn the Select model window, choose Claude 3 Haiku and choose Apply.\n(Note, you may test with other models also as long as you have enabled those models.)\nType the below question in the Enter your message here window.\nWhat are Amazon sustainability goals by year 2040? Choose Run.\nYou see a response shown below.\nExplore the capabilities of your knowledge base by posing diverse questions. A notable feature is its ability to maintain context throughout the conversation. This allows for progressive inquiry, where each question can build upon previous responses, enabling more in-depth and nuanced interactions. Take advantage of this contextual awareness to delve deeper into topics and extract more comprehensive insights from your data.\n"
},
{
	"uri": "//localhost:1313/9-knowledgebases/9.2-integrateknowledgebase/",
	"title": "Integrate Knowledge Base with a Lambda Function",
	"tags": [],
	"description": "",
	"content": "In this task, you will integrate the Amazon Bedrock Knowledge Base with an AWS Lambda function, utilizing the existing API Gateway setup.\nThe focus will be on setting up a Lambda function that interacts with the Knowledge Base and connects to the pre-established API endpoint.\nThis integration will enable the existing UI to communicate with the Knowledge Base through the serverless architecture, allowing for efficient querying and retrieval information with RAG.\nBy leveraging the pre-configured API Gateway and implementing the Lambda function, and complete the backend infrastructure, enabling users to access the power of the Knowledge Base through the familiar web interface, while maintaining the scalability and flexibility of the cloud-native solution.\n⚙️ Setting up AWS Lambda Function The Lambda function was previously deployed as part of the SAM deployment in Task 2.\nIn this task, you will update the function\u0026rsquo;s code and an environment variable containing the Knowledge Base ID.\nThese changes will enable the Lambda function to interact effectively with the Knowledge Base.\nℹ️ Note\nBefore proceeding, ensure the knowledge base creation is complete.\nThis step requires the knowledge base ID, which will be used as a Lambda environment variable.\nVerify the status of the knowledge base you created in the previous task, and only continue once it\u0026rsquo;s fully operational.\nOpen the VSCode editor. From the bedrock-serverless-workshop project, open\n/lambdas/llmFunctions/kbfunction.py, copy the below code and update the function code.\nThis function carries the logic to call knowledge base. import os import json import boto3 import traceback region = boto3.session.Session().region_name KB_ID = os.environ[\u0026#34;KB_ID\u0026#34;] def lambda_handler(event, context): boto3_version = boto3.__version__ print(f\u0026#34;Boto3 version: {boto3_version}\u0026#34;) print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) prompt = event_body[\u0026#34;query\u0026#34;] model_id = event_body[\u0026#34;model_id\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 try: model_arn = \u0026#39;arn:aws:bedrock:\u0026#39;+region+\u0026#39;::foundation-model/\u0026#39;+model_id print(f\u0026#34;Model arn: {model_arn}\u0026#34;) response = retrieveAndGenerate(prompt, model_arn)[\u0026#34;output\u0026#34;][\u0026#34;text\u0026#34;] return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;answer\u0026#39;: response}) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(stack_trace) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } def retrieveAndGenerate(prompt, model_arn): bedrock_agent_runtime = boto3.client( service_name = \u0026#34;bedrock-agent-runtime\u0026#34;) return bedrock_agent_runtime.retrieve_and_generate( input={ \u0026#39;text\u0026#39;: prompt }, retrieveAndGenerateConfiguration={ \u0026#39;type\u0026#39;: \u0026#39;KNOWLEDGE_BASE\u0026#39;, \u0026#39;knowledgeBaseConfiguration\u0026#39;: { \u0026#39;knowledgeBaseId\u0026#39;: KB_ID, \u0026#39;modelArn\u0026#39;: model_arn } } ) Run the following commands to retrieve the Knowledge Base ID and update the Lambda function\u0026rsquo;s environment variable: export KB_ID=$(aws bedrock-agent list-knowledge-bases | jq -r \u0026#39;.knowledgeBaseSummaries[0].knowledgeBaseId\u0026#39;) echo \u0026#34;Knowledge Base ID: $KB_ID\u0026#34; sed -Ei \u0026#34;s|copy_kb_id|${KB_ID}|g\u0026#34; ./template.yaml Open VSCode terminal, run the following command to build and deploy with new updated lambda code. cd ~/environment/bedrock-serverless-workshop sam build \u0026amp;\u0026amp; sam deploy The Lambda function has been successfully integrated with your Amazon Bedrock Knowledge Base.\nThis integration enables the Lambda function to interact directly with the Knowledge Base, allowing it to retrieve and process information from your proprietary data.\n🧪 Test Knowledge Base using UI Return to the browser and open the chatbot webpage.\nIf you are not signed in, use the credentials that you retrieved earlier to sign in.\nChoose RAG with Knowledge Bases option from the menu.\nAsk a question — you can use a sample question from the right side panel.\nHere is a sample question and the response from Claude 3.5 Sonnet.\nYou can try with other LLMs and compare the results.\nWhat are Amazon sustainability goals by year 2040? This task has guided you through the process of creating and integrating an Amazon Bedrock Knowledge Base with AWS Lambda and API Gateway. You\u0026rsquo;ve successfully set up serverless architecture that leverages your organization\u0026rsquo;s proprietary data to enhance AI-driven applications. By connecting your Knowledge Base to Lambda and utilizing the existing API Gateway, you\u0026rsquo;ve created a robust backend capable of processing queries, retrieving relevant information, and generating context-aware responses. This integration opens up numerous possibilities for developing intelligent applications, from advanced chatbots to sophisticated search interfaces, all while maintaining data security and scalability.\n"
},
{
	"uri": "//localhost:1313/9-knowledgebases/",
	"title": "Rag Chat with Bedrock Knowledge Bases",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock Knowledge Bases is a fully managed capability that enables you to implement Retrieval Augmented Generation (RAG) workflows using your organization\u0026rsquo;s proprietary data sources. Amazon Bedrock Knowledge Bases automates the entire RAG workflow, from data ingestion to retrieval and prompt augmentation, without requiring custom integrations or data flow management. This allows you to equip foundation models (FMs) and agents with up-to-date and proprietary information to deliver more relevant and accurate responses.\nAmazon Bedrock Knowledge Bases rely on two key components to enable efficient retrieval of relevant information: embeddings and vector stores. These elements work together to transform text data into a format that can be quickly searched and retrieved based on semantic similarity.\nEmbeddings are numerical representations of text that capture semantic meaning. In Amazon Bedrock, embedding models like Amazon Titan Embeddings or Cohere Embed convert text from your documents into dense vectors. This process allows for efficient comparison and retrieval of semantically similar content, forming the foundation of the knowledge base\u0026rsquo;s understanding of your data.\nVector stores are specialized databases designed to index and query these vector embeddings efficiently. Amazon Bedrock offers options like Amazon-managed OpenSearch Serverless or custom solutions such as Amazon Aurora PostgreSQL with pgvector. These stores enable rapid similarity searches, allowing the system to quickly identify and retrieve the most relevant information when responding to queries, thus enhancing the performance of Retrieval Augmented Generation (RAG) workflows.\nFunctionality and Benefits Seamless RAG Implementation: Amazon Bedrock Knowledge Bases automates the entire RAG workflow, from data ingestion to retrieval and prompt augmentation, without requiring custom integrations or data flow management. This allows you to equip foundation models (FMs) and agents with up-to-date and proprietary information to deliver more relevant and accurate responses.\nSecure Data Connection: The service automatically fetches documents from your specified data sources, including Amazon S3, Web Crawler, Salesforce, and SharePoint. It then processes the content by dividing it into text blocks, converting them into embeddings, and storing them in a vector database.\nCustomization Options: You can fine-tune both retrieval and ingestion processes to improve accuracy across different use cases. Advanced parsing options are available for understanding complex unstructured data, and you can choose from various chunking strategies or even write custom chunking code.\nKnowledge Bases Architecture In the next two tasks, you will build a chatbot using Amazon Bedrock Knowledge Bases. The architecture of the solution is illustrated below: Knowledge Base and Vector Search Components Knowledge Bases: Central repository for structured information Amazon S3: Storage for documents (pdf, csv, txt, etc.) Amazon OpenSearch: Vector database and search engine for efficient similarity search Amazon Bedrock: Provides access to embedding models, including Amazon Titan Embeddings Knowledge Base and Vector Search Workflow Documents are stored in Amazon S3 Text is extracted and processed from documents Amazon Bedrock\u0026rsquo;s Titan Embeddings model generates vector representations of text Vectors are stored in Amazon OpenSearch, functioning as a vector database Knowledge Bases ingest and structure information, incorporating vector representations AWS Lambda functions (RAG/KB/LLM Functions) interact with Knowledge Bases and vector search RAG (Retrieval Augmented Generation) leverages vector similarity search for relevant information retrieval 🔍 Key Features of Knowledge Base and Vector Search Integration Semantic search capabilities using vector representations of text Efficient similarity search through Amazon OpenSearch\u0026rsquo;s vector database functionality Integration of Amazon Titan Embeddings for high-quality text vectorization Enhanced context and accuracy in chatbot responses using vector-based retrieval Improved relevance in information retrieval for RAG operations Ability to handle and search through large volumes of unstructured text data Seamless combination of traditional keyword search and vector-based semantic search This setup allows the chatbot to perform advanced semantic searches on enterprise knowledge.\nBy using Amazon Titan Embeddings to create vector representations of text and storing these in Amazon OpenSearch as a vector database, the system can find contextually similar information even when exact keyword matches are not present.\nThis significantly enhances the chatbot\u0026rsquo;s ability to understand and respond to user queries with relevant information from the enterprise\u0026rsquo;s knowledge base.\n"
},
{
	"uri": "//localhost:1313/10-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "After you\u0026rsquo;ve completed the workshop, to stop incurring costs, you should remove the resources that you created in your AWS account.\nMethod 1: Use AWS CLI to delete resources Remove the AWS SAM application and startup CloudFormation stack Navigate to the AWS VScode terminal, and then run the following commands:\nTo delete SAM stack, run the following command. cd ~/environment/bedrock-serverless-workshop sam delete To delete startup stack, run the following command. aws cloudformation delete-stack --stack-name $CFNStackName Method 2: Manually delete resources from the AWS Console Delete OpenSearch Collection Go to Amazon OpenSearch Service in the AWS Console.\nClick Collections under Serverless in the sidebar.\nSelect the collection → click Delete → confirm.\nDelete CloudFormation Stack Go to AWS CloudFormation in the AWS Console.\nFind the stack named chatbot-startup-stack.\nSelect it → click Delete → confirm.\nDelete a Knowledge Base "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]