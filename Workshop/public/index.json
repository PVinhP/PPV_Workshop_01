[
{
	"uri": "//localhost:1313/",
	"title": "Create a Serverless Chatbot Using Amazon Bedrock, Amazon Kendra, and Your Own Data",
	"tags": [],
	"description": "",
	"content": "Create a Serverless Chatbot Using Amazon Bedrock, Amazon Kendra, and Your Own Data Overall In this workshop, you will embark on a comprehensive journey to create a serverless chatbot using cutting-edge generative AI technologies. You\u0026rsquo;ll leverage Amazon Bedrock, Amazon Kendra, and your own data to build an intelligent conversational interface that can retrieve and generate contextually relevant responses. The workshop will guide you through configuring multiple services including AWS Lambda, API Gateway, and AWS Amplify, and you\u0026rsquo;ll work with various large language models like Claude 3, Mistral, and Llama. By the end of the workshop, you\u0026rsquo;ll have a fully functional chatbot that can interact with enterprise knowledge bases using Retrieval-Augmented Generation (RAG) techniques. This workshop addresses real-world use cases where organizations need intelligent, data-driven conversational interfaces.\nYou\u0026rsquo;ll learn how to solve challenges such as enterprise knowledge discovery, customer support automation, and information retrieval across complex document repositories. The solution you\u0026rsquo;ll build can be applied in numerous business scenarios, including technical support systems, HR knowledge management, compliance document querying, and research information synthesis. By mastering these techniques, you\u0026rsquo;ll be equipped to create AI-powered solutions that can transform how businesses interact with their data, providing faster, more accurate, and context-aware responses to complex queries.\nYou will create the following architecture for this workshop:\nContent Introduction Preparation Connect to EC2 instance Manage session logs Port Forwarding Clean up resources "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Overall In this workshop, you will embark on a comprehensive journey to create a serverless chatbot using cutting-edge generative AI technologies. You\u0026rsquo;ll leverage Amazon Bedrock, Amazon Kendra, and your own data to build an intelligent conversational interface that can retrieve and generate contextually relevant responses. The workshop will guide you through configuring multiple services including AWS Lambda, API Gateway, and AWS Amplify, and you\u0026rsquo;ll work with various large language models like Claude 3, Mistral, and Llama. By the end of the workshop, you\u0026rsquo;ll have a fully functional chatbot that can interact with enterprise knowledge bases using Retrieval-Augmented Generation (RAG) techniques. This workshop addresses real-world use cases where organizations need intelligent, data-driven conversational interfaces.\nYou\u0026rsquo;ll learn how to solve challenges such as enterprise knowledge discovery, customer support automation, and information retrieval across complex document repositories. The solution you\u0026rsquo;ll build can be applied in numerous business scenarios, including technical support systems, HR knowledge management, compliance document querying, and research information synthesis. By mastering these techniques, you\u0026rsquo;ll be equipped to create AI-powered solutions that can transform how businesses interact with their data, providing faster, more accurate, and context-aware responses to complex queries.\nYou will create the following architecture for this workshop:\nüß© Components Users: End-users of the chatbot Web UI: User interface for chatbot interaction AWS Amplify (React, Vue.js): Manages front-end and authentication, specifically using React or Vue.js Amazon API Gateway: Handles API requests AWS Lambda (RAG/KB/LLM Functions): Executes serverless functions for Retrieval-Augmented Generation, Knowledge Base operations, and LLM interactions Amazon Bedrock: Provides access to AI models and services Large Language Models (Claude 3, Mistral, Llama etc.): AI models powering responses Knowledge Bases: Stores structured information Amazon S3: Object storage for documents and data Documents (PDF, CSV, TXT etc.): Various file types for ingestion Amazon Kendra: Intelligent search service Amazon OpenSearch: Vector database and search engine for efficient similarity search Amazon Cognito: User authentication and authorization üîÑ Workflow Users interact with the Web UI Requests routed through API Gateway to Lambda functions Lambda functions use Bedrock for LLM access, RAG operations, and Knowledge Base interactions Knowledge retrieved from Knowledge Bases, S3, Kendra, or OpenSearch System incorporates various document types to enhance the knowledge base ‚ú® Key Features Scalable, serverless architecture Leverages various LLM models including Claude 3, Mistral, and Llama Incorporates enterprise knowledge through Kendra and custom knowledge bases Secure authentication with Cognito Flexible document ingestion and search capabilities Front-end built with modern frameworks (React or Vue.js) using AWS Amplify This architecture enables a generative AI-powered chatbot solution that can scale with demand while providing intelligent responses based on both LLMs\u0026rsquo; knowledge and enterprise-specific knowledge. The use of React or Vue.js with AWS Amplify ensures a responsive and efficient user interface.\nAmazon Kendra Amazon Kendra is a managed information retrieval and intelligent search service that uses natural language processing and advanced deep learning model. Unlike traditional keyword-based search, Amazon Kendra uses semantic and contextual similarity‚Äîand ranking capabilities‚Äîto decide whether a text chunk or document is relevant to a retrieval query.\nSource: Amazon Kendra ‚Äì What is Kendra?\nThis source provides a more detailed explanation of how Amazon Kendra works.\nAmazon Bedrock Amazon Bedrock is a fully managed service that makes high-performing foundation models (FMs) from leading AI companies and Amazon available for your use through a unified API. You can choose from a wide range of foundation models to find the model that is best suited for your use case. Amazon Bedrock also offers a broad set of capabilities to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top foundation models for your use cases, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources.\nWith Amazon Bedrock\u0026rsquo;s serverless experience, you can get started quickly, privately customize foundation models with your own data, and easily and securely integrate and deploy them into your applications using AWS tools without having to manage any infrastructure.\nSource: Amazon Bedrock ‚Äì What is Amazon Bedrock?\nThis source provides a more detailed explanation of how Amazon Bedrock works.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-launchastack/",
	"title": "Launch a CloudFormation stack",
	"tags": [],
	"description": "",
	"content": "\rSupported Regions: We recommended that you run this workshop in the us-west-2 AWS Region.\nAn AWS CloudFormation template is used to set up lab resources in the AWS Region that you choose. This step is required because later instructions are based on these resources. The CloudFormation template creates the following AWS resources:\nVSCode: VSCode on Amazon EC2 is a cloud-based integrated development environment (IDE) that you can use to write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. In this workshop, you use VSCode editor to deploy a backend application, which is built by using AWS Serverless Application Model (AWS SAM), and also deploy AWS Amplify frontend. Amazon S3: Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can store and protect any amount of data for virtually any use case, such as data lakes, cloud-centered applications, and mobile apps. In this workshop, you use an S3 bucket to upload your use case-centric documents. Amazon Kendra indexes these documents to provide Retrieval-Augmented Generation (RAG) answers to questions that you ask. Amazon Kendra: Amazon Kendra is an intelligent enterprise search service that helps you search across different content repositories with built-in connectors. Download the CloudFormation template: Download the CloudFormation template: Download Store the YAML template file in a folder on your local machine. Navigate to AWS CloudFormation Console üîó On the CloudFormation console, choose Upload a template file. Select the template that you just downloaded, and then choose Next Give the stack a name, such as chatbot-startup-stack\nKeep other values unchanged, and choose Next\nFor Configure stack options, select I acknowledge\u0026hellip; options and choose Next\nTo deploy the template, choose Submit After the template is deployed, to review the created resources, navigate to CloudFormation Resources, and then select the CloudFormation stack that you created.\n‚è≥ Template deployment takes 10‚Äì15 minutes to complete all AWS resource provisioning.\nCongratulations! You can now proceed to the next task.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-launchvscode/",
	"title": "Launch VSCode on AWS, and Set Up the Environment",
	"tags": [],
	"description": "",
	"content": "Launch VSCode on AWS To set up your environment, open the VSCode environment (a replacement for AWS Cloud9) which is hosted on Amazon EC2.\nOpen AWS CloudFormation console. Open chatbot-startup-stack stack. Open Outputs, Copy VSCodeWorkspaceURL and VSCodeWorkspacePassword password in a notepad. 4. Open a new browser window or tab, enter the VSCode workspace url, and enter password to launch the VSCode editor.\nAfter it is successfully launched, the default theme is white, optionally you can change to different color themes. For example, Settings Icon -\u0026gt; Themes -\u0026gt; Color Theme -\u0026gt; Dark (Visual Studio)\nYour VSCode editor is ready. Congratulations! You can now proceed to next task.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "\rIt is recommended to use an IAM user account with administrative privileges rather than the root account. Some AWS services, such as Knowledge Bases in Amazon Bedrock or other AI-related services, may be restricted or unavailable when using the root account. For security and compatibility reasons, AWS best practices also advise avoiding the use of the root user for daily tasks.\nSupported Regions: We recommended that you run this workshop in the us-west-2 AWS Region.\nIn this workshop, you configure the following AWS services to build a generative AI chatbot.\nAmazon Bedrock Knowledge Bases Amazon Kendra AWS Lambda An AWS CloudFormation template is used to set up lab resources in the AWS Region that you choose. This step is required because later instructions are based on these resources\nCost: Be aware that if you run this workshop in your own AWS account, you will incur costs for resource usage. The Clean Up section in the left navigation pane can help you remove resources from your environment when you are done.\nContent 2.1 Launch a CloudFormation stack 2.2 Launch VSCode on AWS, and Set Up the Environment "
},
{
	"uri": "//localhost:1313/3-accessibilitytoinstances/",
	"title": "Deploy Amazon Bedrock Serverless Application",
	"tags": [],
	"description": "",
	"content": "In this task, you will build and deploy both the backend and frontend components of the application. The backend is deployed as a serverless application using AWS SAM, which creates an Amazon API Gateway, the necessary AWS Lambda functions, a Cognito user pool, and stores login credentials in AWS Secrets Manager.\nThe frontend is built using Vue.js and deployed using AWS Amplify. The frontend UI will call REST-based API calls hosted using Amazon API Gateway, API Gateway invokes AWS Lambda function, function calls Amazon Bedrock APIs. This serverless architecture allows the application to scale efficiently and reduces the operational overhead of managing the underlying infrastructure. The deployment process is automated through a startup.sh script, which handles the end-to-end provisioning of the entire application stack.\nBuild and deploy the chatbot application From the VSCode editor, run the following command to set a key environment variable used throughout the workshop. Replace \u0026lt;chatbot-startup-stack\u0026gt; with the actual stack name from the AWS CloudFormation console.\nThis CFNStackName variable will be referenced later when interacting with the resources provisioned as part of app deployment. And, the environment variable S3BucketName will be used throughout the workshop to store various data sources and knowledge bases required for the application. The backend services will interact with the contents of this S3 bucket to retrieve and process the necessary information for the application\u0026rsquo;s functionality. Ensuring the S3BucketName environment variable is properly set will allow the workshop tasks to seamlessly access and utilize the required data stored in this central location.\nexport CFNStackName=\u0026#34;chatbot-startup-stack\u0026#34; export S3BucketName=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;S3BucketName\u0026#39;].OutputValue\u0026#34; --output text) echo \u0026#34;S3 bucket name: $S3BucketName\u0026#34; To clone the source code for this workshop, run the following command. The application code is hosted in the open-source aws-samples GitHub repository, which contains a variety of sample projects provided by AWS. Cloning the repository will ensure you have the latest version of the code and can easily make any modifications as you progress through the workshop tasks.\ncd ~/environment git clone https://github.com/aws-samples/bedrock-serverless-workshop.git To build and deploy backend and frontend of serverless app, run the following command. The aws-creds.py script is used to create an AWS profile, which is a required step for the subsequent frontend deployment process..\ncd ~/environment/bedrock-serverless-workshop python3 aws-creds.py chmod +x startup.sh ./startup.sh The script is doing following tasks:\nUpgrade OS, install jq software. Build backend using sam build. Deploy backend using sam deploy. Install Amplify and build frontend. Publish the frontend application using Amplify. The script will take anywhere from 2 to 5 minutes to finish. If there is a git alert popup window at some point, choose OK.\nDuring amplify add host, you are prompted with a selection twice, keep the default selections and hit enter. The defaults are, Hosting with Amplify Console (Managed hosting with custom domains, Continuous deployment) for the first option, and Manual deployment for the second option.\nAfter completion, you will see the following image.\nCopy and store the value for amplifyapp url, user_id, and password in a text editor. You use these credentials to sign in to the UI.\nLaunch the amplifyapp url from the web browser, login using above credentials. After a successful login you see the the below home screen. Note, it is not yet ready with source documents, and the chat is not yet functional. In the next task you complete indexing your source documents and test with sample questions. Before you move to next task, run below commands, these are the environment variables required for SAM and Amplify build commands for rest of the lab.\nRun the following commands in the VS Code terminal.\nexport SAMStackName=\u0026#34;sam-$CFNStackName\u0026#34; export AWS_REGION=$(aws configure get region) export KendraIndexID=$(aws cloudformation describe-stacks --stack-name ${CFNStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;KendraIndexID\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) export BedrockApiUrl=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BedrockApiUrl\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CognitoUserPool\u0026#39;].OutputValue\u0026#34; --output text) export UserPoolClientId=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CongnitoUserPoolClientID\u0026#39;].OutputValue\u0026#34; --output text) export SecretName=$(aws cloudformation describe-stacks --stack-name ${SAMStackName} --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;SecretsName\u0026#39;].OutputValue\u0026#34; --output text) export PATH=~/.npm-global/bin:$PATH After running, you can check again by using the export command and verifying whether the above variables(AWS_REGION, KendraIndexID, BedrockApiUrl,\u0026hellip;) have been assigned values. If the variables do not have values, check your region to ensure it is set to the us-west-2 AWS Region. If it\u0026rsquo;s incorrect, delete the stack, select the correct region, and run the steps again.\nYou have completed the deployment of the backend and frontend of the chatbot using Amazon Bedrock serverless\n"
},
{
	"uri": "//localhost:1313/4-walkthroughbedrock/",
	"title": "Walkthrough Amazon Bedrock Console",
	"tags": [],
	"description": "",
	"content": "Use Amazon Bedrock to build and scale generative AI applications with FMs. Amazon Bedrock is a fully managed service that makes FMs, from leading AI startups and Amazon, available through an API. You can choose from a wide range of FMs to find the model that is best suited for your use case. With the Amazon Bedrock serverless experience, you can get started quickly, privately customize FMs with your own data, and quickly integrate and deploy them into your applications by using AWS tools without having to manage any infrastructure.\nOpen Amazon Bedrock console Activate model access To complete this workshop, you need to enable access to Claude3, Mistral, and Llama models. Additionally, verify that the Titan Text Embeddings V2 model is enabled; this model is typically available by default, but confirm its status and enable it if necessary. Please ensure you have access to all these required models before proceeding with the workshop.\nAt the upper-left of the Amazon Bedrock console, choose the menu (hamburger) icon. In the navigation pane, choose Model access, and then select Enable specific models. Select the following LLMs: Titan Text Embeddings V2 Claude 3.5 Sonnet Claude 3 Haiku Claude 3 Opus Llama 3.1 8B Instruct 8B Mistral 7B Instruct Choose Next Choose Submit In the current version of this workshop environment, the use of the Claude 3 Opus model is not supported. If you encounter any errors related to Opus, you can safely ignore them and proceed without testing the Opus model. However, if you choose to deploy this application in your own AWS account, you will be able to enable the Opus model without any issues.\nAsk a question At the upper-left of the Amazon Bedrock console, choose the menu (hamburger) icon.\nUnder Test, choose Chat / Text playground.\nYou can also take this time, on the left menu, to navigate around the different options.\nOn the Chat / Text playground, choose Select model. In the Select model popup, choose Anthropic and select Claude 3 Haiku. Choose Apply.\nIn the prompt text box, type question, and then choose Run.\nSample question ‚Äì List top 10 cities of VietNam by population?\nThe model returns a response similar to this:\nIn the Configurations, You can try randomness by changing parameter like temperature, Top P, Top K, and token length. You can also try with a different questions and experience with different response behavior.\nTemperature is a parameter that controls the level of randomness in the output; the higher the value, the more creative the model becomes. Top P is the cumulative probability threshold for selecting candidate tokens, helping balance between logic and diversity. Top K is the number of highest-probability tokens considered for the next word selection. Max token length is the maximum number of tokens the model will generate in a response to control the output length and cost. üéâ Congratulations You can now proceed to the next task.\n"
},
{
	"uri": "//localhost:1313/5-multiplellms/",
	"title": "General Chat with Multiple LLMs",
	"tags": [],
	"description": "",
	"content": "\rPort Forwarding is a useful way to redirect network traffic from one IP address - Port to another IP address - Port. With Port Forwarding we can access an EC2 instance located in the private subnet from our workstation.\nYou have successfully deployed the Amazon Bedrock serverless chatbot application, using AWS SAM for the backend and AWS Amplify for the frontend. The frontend serves as a basic user interface for testing the solution with various questions and prompt parameters. In this section, you will update and deploy a LLM Lambda function, this function enables with a general chat with multiple large language models.\nOpen the VSCode editor (as discussed in Task 1). From bedrock-serverless-workshop project, open /lambdas/llmFunctions/llmfunction.py function, copy the below code and update the function code. This function carries the logic to support Claud3 (Haiku, Sonnet etc.), Mistral and Llama models. import boto3 import json import traceback region = boto3.session.Session().region_name def lambda_handler(event, context): boto3_version = boto3.__version__ print(f\u0026#34;Boto3 version: {boto3_version}\u0026#34;) print(f\u0026#34;Event is: {event}\u0026#34;) event_body = json.loads(event[\u0026#34;body\u0026#34;]) prompt = event_body[\u0026#34;query\u0026#34;] temperature = event_body[\u0026#34;temperature\u0026#34;] max_tokens = event_body[\u0026#34;max_tokens\u0026#34;] model_id = event_body[\u0026#34;model_id\u0026#34;] response = \u0026#39;\u0026#39; status_code = 200 try: if model_id == \u0026#39;mistral.mistral-7b-instruct-v0:2\u0026#39;: response = invoke_mistral_7b(model_id, prompt, temperature, max_tokens) elif model_id == \u0026#39;meta.llama3-1-8b-instruct-v1:0\u0026#39;: response = invoke_llama(model_id, prompt, temperature, max_tokens) else: response = invoke_claude(model_id, prompt, temperature, max_tokens) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;answer\u0026#39;: response}) } except Exception as e: print(f\u0026#34;An unexpected error occurred: {str(e)}\u0026#34;) stack_trace = traceback.format_exc() print(stack_trace) return { \u0026#39;statusCode\u0026#39;: status_code, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;OPTIONS,POST\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } def invoke_claude(model_id, prompt, temperature, max_tokens): try: instruction = f\u0026#34;Human: {prompt} nAssistant:\u0026#34; bedrock_runtime_client = boto3.client(service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=region) body= { \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: prompt}], } ], } response = bedrock_runtime_client.invoke_model( modelId=model_id, body=json.dumps(body) ) response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) outputs = response_body.get(\u0026#34;content\u0026#34;) completions = [output[\u0026#34;text\u0026#34;] for output in outputs] print(f\u0026#34;completions: {completions[0]}\u0026#34;) return completions[0] except Exception as e: raise def invoke_mistral_7b(model_id, prompt, temperature, max_tokens): try: instruction = f\u0026#34;\u0026lt;s\u0026gt;[INST] {prompt} [/INST]\u0026#34; bedrock_runtime_client = boto3.client(service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=region) body = { \u0026#34;prompt\u0026#34;: instruction, \u0026#34;max_tokens\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, } response = bedrock_runtime_client.invoke_model( modelId=model_id, body=json.dumps(body) ) response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) outputs = response_body.get(\u0026#34;outputs\u0026#34;) print(f\u0026#34;response: {outputs}\u0026#34;) completions = [output[\u0026#34;text\u0026#34;] for output in outputs] return completions[0] except Exception as e: raise def invoke_llama(model_id, prompt, temperature, max_tokens): print(f\u0026#34;Invoking llam model {model_id}\u0026#34; ) print(f\u0026#34;max_tokens {max_tokens}\u0026#34; ) try: instruction = f\u0026#34;[INST]You are a very intelligent bot with exceptional critical thinking, help me answering below question.[/INST]\u0026#34; total_prompt = f\u0026#34;{instruction}\\n{prompt}\u0026#34; print(f\u0026#34;Prompt template {total_prompt}\u0026#34; ) bedrock_runtime_client = boto3.client(service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=region) body = { \u0026#34;prompt\u0026#34;: total_prompt, \u0026#34;max_gen_len\u0026#34;: max_tokens, \u0026#34;temperature\u0026#34;: temperature, \u0026#34;top_p\u0026#34;: 0.9 } response = bedrock_runtime_client.invoke_model( modelId=model_id, body=json.dumps(body) ) response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) print(f\u0026#34;response: {response_body}\u0026#34;) return response_body [\u0026#39;generation\u0026#39;] except Exception as e: raise From VSCode editor, run the following command to build and deploy with new updated lambda code. cd ~/environment/bedrock-serverless-workshop sam build \u0026amp;\u0026amp; sam deploy You can inspect the updated Lambda function using AWS Lambda Console.\nIn this task, you have the opportunity to work with following LLM models, each offering unique functionalities and specializations. These models include:\nanthropic.claude-3-haiku - Claude 3 Haiku is Anthropic‚Äôs fastest, most compact model for near-instant responsiveness. Haiku is the best choice for building seamless AI experiences that mimic human interactions. anthropic.claude-3-5-sonnet - Anthropic‚Äôs most intelligent and advanced model, Claude 3.5 Sonnet, demonstrates exceptional capabilities across a diverse range of tasks and evaluation. anthropic.claude-3-opus - Opus is a highly intelligent model with reliable performance on complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Use Opus to automate tasks, and accelerate research and development across a diverse range of use cases and industries. mistral.mistral-7b-instruct - Mistral is a highly efficient large language model optimized for high-volume, low-latency language-based tasks. Popular use cases for Mistral are text summarization, structuration, question answering, and code completion meta.llama3-1-8b-instruct - Llama 3.1 8B is best suited for limited computational power and resources. The model excels at text summarization, text classification, sentiment analysis, and language translation requiring low-latency inferencing. Test using UI Return to the browser and open the chatbot webpage. If you are not signed in, use the credentials that you retrieved earlier to sign in. The webpage is displayed as follows: 2. Ask any question by typing in the Query text box and click Ask Question button. You can try with a sample question from the right side panel. Here is a sample question and the response from Claude 3 Haiku. You can try with other LLMs and compare the results.\nList top 10 cities of VietNam by population? Task Complete You can now proceed to next task.\n"
},
{
	"uri": "//localhost:1313/6-usingamazonkendra/",
	"title": "Index the Source Data by Using Amazon Kendra",
	"tags": [],
	"description": "",
	"content": "In the RAG architecture, Amazon Kendra can be leveraged to index and search through a collection of sample documents stored in Amazon S3 and other sources. Users can provide a query or question, and Kendra will perform a similarity search across the indexed content to identify the most relevant information.\nKendra\u0026rsquo;s advanced natural language processing capabilities allow it to understand the user\u0026rsquo;s intent and query, and then retrieve the most pertinent content from the indexed data sources. This enables users to quickly find the information they need, without having to manually sift through large volumes of documents.\nBy integrating Kendra into the \u0026ldquo;Retrieve\u0026rdquo; phase of the RAG architecture, organizations can enhance their overall search and information discovery capabilities, ultimately supporting more effective analysis and generation of insights and responses. Kendra\u0026rsquo;s seamless integration with Amazon S3 simplifies the process of indexing and managing the underlying content, making it a powerful tool within the RAG framework.\nDownload sample documents Download a few sample documents to test this solution. The first document pertains to the May and September 2024 meeting minutes of the Federal Open Market Committee (FOMC). The second document is the 2023 Amazon Sustainability Report. The third document is the 2023 10K report for Henry Schein, a provider of dental services. You can download or use any document to test this solution, or you can bring your own data to conduct tests.\nTo copy the prompt templates and sample documents that you downloaded to the S3 bucket, run the following command.\nOpen the VSCode editor, then run the command in the TERMINAL.\ncd ~/environment/bedrock-serverless-workshop mkdir sample-documents curl https://www.federalreserve.gov/monetarypolicy/files/monetary20240501a1.pdf --output sample-documents/monetary20240501a1.pdf curl https://www.federalreserve.gov/monetarypolicy/files/monetary20240918a1.pdf --output sample-documents/monetary20240918a1.pdf curl https://sustainability.aboutamazon.com/content/dam/sustainability-marketing-site/pdfs/reports-docs/2023-amazon-sustainability-report.pdf --output sample-documents/2023-sustainability-report-amazon.pdf curl https://investor.henryschein.com/static-files/bcc116aa-a576-4756-a722-90f5e2e22114 --output sample-documents/2023-hs1-10k.pdf Upload sample documents and prompt templates To copy prompt templates and sample documents you downloaded to the S3 bucket, run the following command.\ncd ~/environment/bedrock-serverless-workshop aws s3 cp sample-documents s3://$S3BucketName/sample-documents/ --recursive aws s3 cp prompt-engineering s3://$S3BucketName/prompt-engineering/ --recursive After a successful upload, review the Amazon S3 console and open the bucket. You should see something like this:\n"
},
{
	"uri": "//localhost:1313/6-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nDelete EC2 instance Go to EC2 service management console\nClick Instances. Select both Public Linux Instance and Private Windows Instance instances. Click Instance state. Click Terminate instance, then click Terminate to confirm. Go to IAM service management console\nClick Roles. In the search box, enter SSM. Click to select SSM-Role. Click Delete, then enter the role name SSM-Role and click Delete to delete the role. Click Users. Click on user Portfwd. Click Delete, then enter the user name Portfwd and click Delete to delete the user. Delete S3 bucket Access System Manager - Session Manager service management console.\nClick the Preferences tab. Click Edit. Scroll down. In the section S3 logging. Uncheck Enable to disable logging. Scroll down. Click Save. Go to S3 service management console\nClick on the S3 bucket we created for this lab. (Example: lab-fcj-bucket-0001 ) Click Empty. Enter permanently delete, then click Empty to proceed to delete the object in the bucket. Click Exit. After deleting all objects in the bucket, click Delete\nEnter the name of the S3 bucket, then click Delete bucket to proceed with deleting the S3 bucket. Delete VPC Endpoints Go to VPC service management console Click Endpoints. Select the 4 endpoints we created for the lab including SSM, SSMMESSAGES, EC2MESSAGES, S3GW. Click Actions. Click Delete VPC endpoints. In the confirm box, enter delete.\nClick Delete to proceed with deleting endpoints. Click the refresh icon, check that all endpoints have been deleted before proceeding to the next step.\nDelete VPC Go to VPC service management console\nClick Your VPCs. Click on Lab VPC. Click Actions. Click Delete VPC. In the confirm box, enter delete to confirm, click Delete to delete Lab VPC and related resources.\n"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]